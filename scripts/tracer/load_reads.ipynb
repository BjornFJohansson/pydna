{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Bio.SeqIO import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.ab1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-235f81ac280b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.ab1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"abi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bjorn38/lib/python3.8/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \"\"\"\n\u001b[0;32m--> 654\u001b[0;31m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bjorn38/lib/python3.8/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0miterator_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FormatToIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAlignIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FormatToIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# Use Bio.AlignIO to read in the alignments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bjorn38/lib/python3.8/site-packages/Bio/SeqIO/AbiIO.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, trim)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;34m\"\"\"Return an iterator for the Abi file format.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ABI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bjorn38/lib/python3.8/site-packages/Bio/SeqIO/Interfaces.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, alphabet, mode, fmt)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The alphabet argument is no longer supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_close_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# not a path, assume we received a stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.ab1'"
     ]
    }
   ],
   "source": [
    "s = read(\"test.ab1\", \"abi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f4d5d0c0671b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "file_obj = open(\"test.ab1\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "read(file_obj, \"abi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('entry ID:', entry['did'])? (<ipython-input-3-40666914c7cd>, line 709)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-40666914c7cd>\"\u001b[0;36m, line \u001b[0;32m709\u001b[0m\n\u001b[0;31m    print 'entry ID:', entry['did']\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('entry ID:', entry['did'])?\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# Copyright (C) 2014 Brian J. Stucky\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "\n",
    "from struct import unpack, pack\n",
    "import struct\n",
    "import zlib\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TraceFileError(Exception):\n",
    "    pass\n",
    "\n",
    "class UnknownFileTypeError(TraceFileError):\n",
    "    def __str__(self):\n",
    "        return 'The file format was not recognized.  Please convert the file to a supported format and try again.'\n",
    "\n",
    "\n",
    "# constants for sequence trace file types\n",
    "ST_UNKNOWN = 0\n",
    "ST_ZTR = 1\n",
    "ST_ABI = 2\n",
    "ST_SCF = 3\n",
    "\n",
    "class SequenceTraceFactory:\n",
    "    @staticmethod\n",
    "    def getTraceFileType(filename):\n",
    "        try:\n",
    "            tf = open(filename, 'rb')\n",
    "        except IOError:\n",
    "            raise\n",
    "    \n",
    "        # read the \"magic number\" from the file\n",
    "        magicval = tf.read(8)\n",
    "        tf.close()\n",
    "        #print magicval\n",
    "    \n",
    "        if magicval[0:4] == 'ABIF':\n",
    "            return ST_ABI\n",
    "        elif magicval == '\\256ZTR\\r\\n\\032\\n':\n",
    "            return ST_ZTR\n",
    "        elif magicval[0:4] == '.scf':\n",
    "            return ST_SCF\n",
    "        else:\n",
    "            return ST_UNKNOWN\n",
    "\n",
    "    @staticmethod\n",
    "    def loadTraceFile(filepath):\n",
    "        try:\n",
    "            ftype = SequenceTraceFactory.getTraceFileType(filepath)\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "        if ftype == ST_ZTR:\n",
    "            seqt = ZTRSequenceTrace()\n",
    "        elif ftype == ST_ABI:\n",
    "            seqt = ABISequenceTrace()\n",
    "        elif ftype == ST_SCF:\n",
    "            seqt = SCFSequenceTrace()\n",
    "        elif ftype == ST_UNKNOWN:\n",
    "            raise UnknownFileTypeError\n",
    "\n",
    "        seqt.loadFile(filepath)\n",
    "\n",
    "        return seqt\n",
    "\n",
    "\n",
    "# Define the reverse complement lookup table.\n",
    "rclookup = {\n",
    "    'a': 't', 't': 'a', 'g': 'c', 'c': 'g',\n",
    "    'w': 'w', 's': 's', 'm': 'k', 'k': 'm', 'r': 'y', 'y': 'r',\n",
    "    'b': 'v', 'd': 'h', 'h': 'd', 'v': 'b', 'n': 'n',\n",
    "    'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G',\n",
    "    'W': 'W', 'S': 'S', 'M': 'K', 'K': 'M', 'R': 'Y', 'Y': 'R',\n",
    "    'B': 'V', 'D': 'H', 'H': 'D', 'V': 'B', 'N': 'N'}\n",
    "\n",
    "def reverseCompSequence(sequence):\n",
    "    \"\"\"\n",
    "    Defines a generic method for reverse complementing a sequence of\n",
    "    nucleotide codes.  This method fully supports all of the IUPAC\n",
    "    ambiguity codes.\n",
    "    \"\"\"\n",
    "    tmp = list()\n",
    "    for cnt in reversed(range(len(sequence))):\n",
    "        tmp.append(rclookup[sequence[cnt]])\n",
    "\n",
    "    return ''.join(tmp)\n",
    "\n",
    "\n",
    "\n",
    "class SequenceTrace:\n",
    "    \"\"\"\n",
    "    Parent for all format-specific sequence trace classes.  This class defines\n",
    "    the methods that are common to all sequence traces.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.isreverse_comped = False\n",
    "        self.fname = ''\n",
    "        self.tracesamps = {}\n",
    "        self.max_traceval = -1\n",
    "        self.comments = {}\n",
    "\n",
    "    def loadFile(self, filename):\n",
    "        pass\n",
    "\n",
    "    def getFileName(self):\n",
    "        return os.path.basename(self.fname)\n",
    "\n",
    "    def getMaxTraceVal(self):\n",
    "        return self.max_traceval\n",
    "\n",
    "    def reverseComplement(self):\n",
    "        \"\"\"\n",
    "        Reverse complements the trace data, including the actual sequencing traces,\n",
    "        the base calls, and the quality scores.\n",
    "        \"\"\"\n",
    "        # reverse the DNA sequence\n",
    "        self.basecalls = reverseCompSequence(self.basecalls)\n",
    "\n",
    "        # reverse and transpose the trace samples\n",
    "        for base in self.tracesamps:\n",
    "            self.tracesamps[base].reverse()\n",
    "        tmp = self.tracesamps['A']\n",
    "        self.tracesamps['A'] = self.tracesamps['T']\n",
    "        self.tracesamps['T'] = tmp\n",
    "        tmp = self.tracesamps['G']\n",
    "        self.tracesamps['G'] = self.tracesamps['C']\n",
    "        self.tracesamps['C'] = tmp\n",
    "\n",
    "        # reverse the confidence scores\n",
    "        self.bcconf.reverse()\n",
    "\n",
    "        # reverse and shift the base call positions\n",
    "        self.basepos.reverse()\n",
    "        endsamp = len(self.tracesamps['A']) - 1\n",
    "        for cnt in range(len(self.basepos)):\n",
    "            self.basepos[cnt] = endsamp - self.basepos[cnt]\n",
    "\n",
    "        self.isreverse_comped = not(self.isreverse_comped)\n",
    "\n",
    "    def isReverseComplemented(self):\n",
    "        return self.isreverse_comped\n",
    "\n",
    "    def getTraceSamples(self, base):\n",
    "        \"\"\"\n",
    "        Returns the actual trace data for a particular base.\n",
    "        \"\"\"\n",
    "        return self.tracesamps[base.upper()]\n",
    "\n",
    "    def getTraceSample(self, base, index):\n",
    "        \"\"\"\n",
    "        Return the magnitude of the trace data at the location in the trace\n",
    "        specified by index.\n",
    "        \"\"\"\n",
    "        return self.tracesamps[base.upper()][index]\n",
    "\n",
    "    def getTraceLength(self):\n",
    "        return len(self.tracesamps['A'])\n",
    "\n",
    "    def getBaseCalls(self):\n",
    "        return self.basecalls\n",
    "\n",
    "    def getBaseCall(self, index):\n",
    "        return self.basecalls[index]\n",
    "\n",
    "    def getNumBaseCalls(self):\n",
    "        return len(self.basecalls)\n",
    "\n",
    "    def getBaseCallPos(self, index):\n",
    "        return self.basepos[index]\n",
    "\n",
    "    def getBaseCallConf(self, index):\n",
    "        return self.bcconf[index]\n",
    "\n",
    "    # If sampnum < the first base call location, returns the first base call location.\n",
    "    def getPrevBaseCallIndex(self, sampnum):\n",
    "        # do a binary search for the index of the base call located at,\n",
    "        # or immediately before, sampnum\n",
    "        minv = 0\n",
    "        maxv = len(self.basepos) - 1\n",
    "\n",
    "        while maxv > (minv + 1):\n",
    "            test = (maxv-minv)/2 + minv\n",
    "            #print 'minv, maxv, test:', minv, maxv, test\n",
    "            if self.basepos[test] == sampnum:\n",
    "                return test\n",
    "            elif self.basepos[test] > sampnum:\n",
    "                maxv = test\n",
    "            else:\n",
    "                minv = test\n",
    "\n",
    "        if self.basepos[maxv] <= sampnum:\n",
    "            return maxv\n",
    "        else:\n",
    "            return minv\n",
    "\n",
    "    # If sampnum > the last base call location, returns the last base call location.\n",
    "    def getNextBaseCallIndex(self, sampnum):\n",
    "        # do a binary search for the index of the base call located at,\n",
    "        # or immediately after, sampnum\n",
    "        minv = 0\n",
    "        maxv = len(self.basepos) - 1\n",
    "\n",
    "        while maxv > (minv+1):\n",
    "            test = (maxv-minv)/2 + minv\n",
    "            #print 'minv, maxv, test:', minv, maxv, test\n",
    "            if self.basepos[test] == sampnum:\n",
    "                return test\n",
    "            elif self.basepos[test] > sampnum:\n",
    "                maxv = test\n",
    "            else:\n",
    "                minv = test\n",
    "\n",
    "        if self.basepos[minv] >= sampnum:\n",
    "            return minv\n",
    "        else:\n",
    "            return maxv\n",
    "\n",
    "    def getComment(self, key):\n",
    "        if key in self.comments:\n",
    "            return self.comments[key]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def getComments(self):\n",
    "        return self.comments\n",
    "\n",
    "\n",
    "class ZTRError(TraceFileError):\n",
    "    pass\n",
    "\n",
    "class ZTRVersionError(ZTRError):\n",
    "    def __init__(self, ver_major, ver_minor):\n",
    "        self.ver_major = ver_major\n",
    "        self.ver_minor = ver_minor\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'This file uses version ' + str(self.ver_major) + '.' + str(self.ver_minor) + ' of the ZTR format.  This software only supports version 1.2 of the format.'\n",
    "\n",
    "class ZTRDataFormatError(ZTRError):\n",
    "    def __init__(self, format_id):\n",
    "        self.format_id = format_id\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'The ZTR data format ID ' + str(self.format_id) + ' is invalid or not supported.'\n",
    "\n",
    "class ZTRMissingDataError(ZTRError):\n",
    "    def __init__(self, expectedlen, actuallen):\n",
    "        self.expectedlen = expectedlen\n",
    "        self.actuallen = actuallen\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Error reading ZTR data chunk.  Expected ' + str(self.expectedlen) + ' bytes but only got ' + str(self.actuallen) + ' bytes.  The file appears to be damaged.'\n",
    "\n",
    "\n",
    "class ZTRSequenceTrace(SequenceTrace):\n",
    "    def loadFile(self, filename):\n",
    "        self.fname = filename\n",
    "\n",
    "        try:\n",
    "            tf = open(filename, 'rb')\n",
    "        except IOError:\n",
    "            raise\n",
    "\n",
    "        # read the header\n",
    "        self.magicnum = tf.read(8)\n",
    "        if self.magicnum != '\\256ZTR\\r\\n\\032\\n':\n",
    "            raise ZTRError('The ZTR file header is invalid.  The file appears to be damaged.')\n",
    "\n",
    "        try:\n",
    "            self.ver_major = unpack('b', tf.read(1))[0]\n",
    "            self.ver_minor = unpack('b', tf.read(1))[0]\n",
    "            #print 'major version number:', ver_major\n",
    "            #print 'minor version number:', ver_minor\n",
    "        except struct.error:\n",
    "            raise ZTRError('The ZTR file header is invalid.  The file appears to be damaged.')\n",
    "\n",
    "        if (self.ver_major != 1) or (self.ver_minor != 2):\n",
    "            raise ZTRVersionError(self.ver_major, self.ver_minor)\n",
    "\n",
    "        # read and process the data chunks\n",
    "        chunk = self.readChunk(tf)\n",
    "        while chunk != False:\n",
    "            #print 'chunk type:', chunk[0]\n",
    "            #print 'compressed data length:', chunk[1]\n",
    "            #print 'uncompressed data length:', len(chunk[2])\n",
    "            if chunk[0] == 'SMP4':\n",
    "                # trace sample data\n",
    "                self.readTraceSamples(chunk[2])\n",
    "            elif chunk[0] == 'BASE':\n",
    "                # base calls\n",
    "                self.basecalls = chunk[2][1:].upper()\n",
    "            elif chunk[0] == 'BPOS':\n",
    "                # positions of base calls relative to trace samples\n",
    "                self.basepos = list()\n",
    "                # skip 4 leading null bytes\n",
    "                for cnt in range(4, len(chunk[2]), 4):\n",
    "                    self.basepos.append(unpack('>I', chunk[2][cnt:cnt+4])[0])\n",
    "            elif chunk[0] == 'CNF4':\n",
    "                # confidence scores; this is required to come after a BASE chunk\n",
    "                self.bcconf = list()\n",
    "                for cnt in range(1, self.getNumBaseCalls()+1):\n",
    "                    self.bcconf.append(unpack('b', chunk[2][cnt])[0])\n",
    "            elif chunk[0] == 'TEXT':\n",
    "                # get the comment key/value strings, ignoring leading/trailing null characters\n",
    "                keyvals = chunk[2][1:-2].split('\\0')\n",
    "                for cnt in range(0, len(keyvals), 2):\n",
    "                    #print keyvals[cnt] + ': ' + keyvals[cnt+1]\n",
    "                    self.comments[keyvals[cnt]] = keyvals[cnt+1]\n",
    "\n",
    "            chunk = self.readChunk(tf)\n",
    "\n",
    "    def readTraceSamples(self, chunkdata):\n",
    "        self.max_traceval = 0\n",
    "        tracelen = (len(chunkdata) - 2) / 4\n",
    "\n",
    "        offset = 2\n",
    "        basenum = 0\n",
    "        for base in ['A','C','G','T']:\n",
    "            thisbase = list()\n",
    "            start = basenum*tracelen + offset\n",
    "            for cnt in range(0, tracelen, 2):\n",
    "                val = unpack('>H', chunkdata[start+cnt:start+cnt+2])[0]\n",
    "                thisbase.append(val)\n",
    "\n",
    "            tmpmax = max(thisbase)\n",
    "            if tmpmax > self.max_traceval:\n",
    "                self.max_traceval = tmpmax\n",
    "            self.tracesamps[base] = thisbase\n",
    "            basenum += 1\n",
    "\n",
    "    def zlibUncompress(self, cdata):\n",
    "        # In examining the Staden package source code, it appears that the 4-byte data length integer is not\n",
    "        # guaranteed to be in big-endian byte order.  Might this be a bug in the Staden code?  For this reason,\n",
    "        # native byte order is used here (in fact, using big-endian order will cause this to fail on an\n",
    "        # x86 machine).\n",
    "        udatalen = unpack('I', cdata[:4])[0]\n",
    "    \n",
    "        udata = zlib.decompress(cdata[4:])\n",
    "\n",
    "        #print 'expected uncompressed data length:', udatalen\n",
    "        #print 'actual uncompressed data length:', len(udata)\n",
    "        if udatalen != len(udata):\n",
    "            raise ZTRError('Zlib decompression failed.  The expected data length did not match the actual data length.')\n",
    "        \n",
    "        return udata\n",
    "    \n",
    "    def RLEUncompress(self, cdata):\n",
    "        # In examining the Staden package source code, it appears that the 4-byte data length integer is not\n",
    "        # guaranteed to be in big-endian byte order.  Might this be a bug in the Staden code?  For this reason,\n",
    "        # native byte order is used here (in fact, using big-endian order will cause this to fail on an\n",
    "        # x86 machine).\n",
    "        udatalen = unpack('I', cdata[:4])[0]\n",
    "        guard = cdata[4]\n",
    "        #print unpack_from('=bIb', data[:6])\n",
    "        #print 'guard byte:', guard\n",
    "    \n",
    "        cnt = 5\n",
    "        udata = list()\n",
    "        while cnt < len(cdata):\n",
    "            if cdata[cnt] == guard:\n",
    "                runlen = unpack('B', cdata[cnt+1])[0]\n",
    "                #print 'run length:', runlen\n",
    "                if runlen == 0:\n",
    "                    udata.append(guard)\n",
    "                    cnt += 2\n",
    "                else:\n",
    "                    runchar = cdata[cnt+2]\n",
    "                    udata.extend(runlen*list(runchar))\n",
    "                    cnt += 3\n",
    "            else:\n",
    "                udata.append(cdata[cnt])\n",
    "                cnt += 1\n",
    "    \n",
    "        #print 'expected uncompressed data length:', udatalen\n",
    "        #print 'actual uncompressed data length:', len(udata)\n",
    "        if udatalen != len(udata):\n",
    "            raise ZTRError('RLE decompression failed.  The expected data length did not match the actual data length.')\n",
    "    \n",
    "        return ''.join(udata)\n",
    "    \n",
    "    def followDecode(self, cdata):\n",
    "        # read the decode table\n",
    "        table = unpack('256B', cdata[:256])\n",
    "        #print table\n",
    "    \n",
    "        udata = list()\n",
    "        prev = unpack('B', cdata[256])[0]\n",
    "        udata.append(cdata[256])\n",
    "        for cnt in xrange(257, len(cdata)):\n",
    "            diff = unpack('b', cdata[cnt])[0]\n",
    "            actual = table[prev] - diff\n",
    "\n",
    "            # simulate 1-byte unsigned overflow/underflow, if needed\n",
    "            if actual < 0:\n",
    "                actual += 256\n",
    "            elif actual > 255:\n",
    "                #print actual,(actual-256), table[prev], diff\n",
    "                actual -= 256\n",
    "\n",
    "            prev = actual\n",
    "            #print actual\n",
    "            udata.append(pack('B', actual))\n",
    "        \n",
    "        return ''.join(udata)\n",
    "    \n",
    "    def decode16To8(self, cdata):\n",
    "        cnt = 0\n",
    "        udata = list()\n",
    "        while cnt < len(cdata):\n",
    "            val = unpack('b', cdata[cnt])[0]\n",
    "            if (val > -128) and (val < 128):\n",
    "                udata.append(pack('>h', val))\n",
    "                cnt += 1\n",
    "            elif val == -128:\n",
    "                #print unpack('b', cdata[cnt+1])[0]\n",
    "                #print unpack('b', cdata[cnt+2])[0]\n",
    "                udata.extend(cdata[cnt+1:cnt+3])\n",
    "                cnt += 3\n",
    "            else:\n",
    "                raise ZTRError('Invalid value encountered while attempting to read 16- to 8-bit encoded ZTR data.')\n",
    "    \n",
    "        return ''.join(udata)\n",
    "    \n",
    "    def decode32To8(self, cdata):\n",
    "        cnt = 0\n",
    "        udata = list()\n",
    "        while cnt < len(cdata):\n",
    "            val = unpack('b', cdata[cnt])[0]\n",
    "            if (val > -128) and (val < 128):\n",
    "                udata.append(pack('>i', val))\n",
    "                cnt += 1\n",
    "            elif val == -128:\n",
    "                #print unpack('b', cdata[cnt+1])[0]\n",
    "                #print unpack('b', cdata[cnt+2])[0]\n",
    "                udata.extend(cdata[cnt+1:cnt+5])\n",
    "                cnt += 5\n",
    "            else:\n",
    "                raise ZTRError('Invalid value encountered while attempting to read 32- to 8-bit encoded ZTR data.')\n",
    "    \n",
    "        return ''.join(udata)\n",
    "    \n",
    "    def decode8BitDelta(self, cdata):\n",
    "        levels = unpack('b', cdata[0])[0]\n",
    "        #print 'levels:', levels\n",
    "    \n",
    "        # first, unpack the 1-byte values\n",
    "        udata = list()\n",
    "        for cnt in xrange(1, len(cdata)):\n",
    "            val = unpack('B', cdata[cnt])[0]\n",
    "            udata.append(val)\n",
    "\n",
    "        # now apply the reverse delta filtering\n",
    "        for clev in range(levels):\n",
    "            prev = 0\n",
    "            for cnt in xrange(0, len(udata)):\n",
    "                actual = udata[cnt] + prev\n",
    "                if actual > 255:\n",
    "                    # simulate 1-byte integer overflow\n",
    "                    actual -= 256\n",
    "                prev = actual\n",
    "                udata[cnt] = actual\n",
    "    \n",
    "        # repack the data\n",
    "        tmpdata = list()\n",
    "        for val in udata:\n",
    "            tmpdata.append(pack('B', val))\n",
    "    \n",
    "        return ''.join(tmpdata)\n",
    "    \n",
    "    def decode16BitDelta(self, cdata):\n",
    "        levels = unpack('b', cdata[0])[0]\n",
    "        #print 'levels:', levels\n",
    "    \n",
    "        # first, unpack the 2-byte values\n",
    "        udata = list()\n",
    "        for cnt in xrange(1, len(cdata), 2):\n",
    "            val = unpack('>H', cdata[cnt:cnt+2])[0]\n",
    "            udata.append(val)\n",
    "\n",
    "        # now apply the reverse delta filtering\n",
    "        for clev in range(levels):\n",
    "            prev = 0\n",
    "            for cnt in xrange(0, len(udata)):\n",
    "                actual = udata[cnt] + prev\n",
    "                if actual > 65535:\n",
    "                    # simulate 2-byte integer overflow\n",
    "                    actual -= 65536\n",
    "                prev = actual\n",
    "                udata[cnt] = actual\n",
    "    \n",
    "        # repack the data\n",
    "        tmpdata = list()\n",
    "        for val in udata:\n",
    "            tmpdata.append(pack('>H', val))\n",
    "    \n",
    "        return ''.join(tmpdata)\n",
    "    \n",
    "    def decode32BitDelta(self, cdata):\n",
    "        levels = unpack('b', cdata[0])[0]\n",
    "        #print 'levels:', levels\n",
    "    \n",
    "        # first, unpack the 4-byte values (skipping the 2 padding bytes)\n",
    "        udata = list()\n",
    "        for cnt in xrange(3, len(cdata), 4):\n",
    "            val = unpack('>I', cdata[cnt:cnt+4])[0]\n",
    "            udata.append(val)\n",
    "\n",
    "        # now apply the reverse delta filtering\n",
    "        for clev in range(levels):\n",
    "            prev = 0\n",
    "            for cnt in xrange(0, len(udata)):\n",
    "                actual = udata[cnt] + prev\n",
    "                if actual > 4294967295:\n",
    "                    # simulate 1-byte integer overflow\n",
    "                    actual -= 4294967296\n",
    "                prev = actual\n",
    "                udata[cnt] = actual\n",
    "    \n",
    "        # repack the data\n",
    "        tmpdata = list()\n",
    "        for val in udata:\n",
    "            tmpdata.append(pack('>I', val))\n",
    "    \n",
    "        return ''.join(tmpdata)\n",
    "    \n",
    "    \n",
    "    def readChunk(self, fp):\n",
    "        # get the chunk descriptor\n",
    "        chtype = fp.read(4)\n",
    "        #print 'chunk type:', chtype\n",
    "\n",
    "        # check for EOF\n",
    "        if len(chtype) == 0:\n",
    "            return False\n",
    "        elif len(chtype) != 4:\n",
    "            raise ZTRError('The ZTR data chunk type could not be read.  The file appears to be damaged.')\n",
    "    \n",
    "        try:\n",
    "            mdlen = unpack('>I', fp.read(4))[0]\n",
    "            #print 'metadata length:', mdlen\n",
    "    \n",
    "            # skip over the metadata\n",
    "            fp.read(mdlen)\n",
    "    \n",
    "            # get the size of the data\n",
    "            datalen = unpack('>I', fp.read(4))[0]\n",
    "            #print 'data length:', datalen\n",
    "        except struct.error:\n",
    "            raise ZTRError('The ZTR data chunk header could not be read.  The file appears to be damaged.')\n",
    "\n",
    "        # read the chunk data from the file\n",
    "        data = fp.read(datalen)\n",
    "        if datalen != len(data):\n",
    "            raise ZTRMissingDataError(datalen, len(data))\n",
    "    \n",
    "        # iteratively process the chunk data until we get the \"raw\",\n",
    "        # uncompressed data\n",
    "        dataformat = unpack('b', data[0])[0]\n",
    "        while dataformat != 0:\n",
    "            #print 'data format:', dataformat\n",
    "            if dataformat == 1:\n",
    "                # run-length encoding\n",
    "                data = self.RLEUncompress(data[1:])\n",
    "            elif dataformat == 2:\n",
    "                # zlib encoding\n",
    "                data = self.zlibUncompress(data[1:])\n",
    "            elif dataformat == 64:\n",
    "                # 8-bit delta encoded\n",
    "                data = self.decode8BitDelta(data[1:])\n",
    "            elif dataformat == 65:\n",
    "                # 16-bit delta encoded\n",
    "                data = self.decode16BitDelta(data[1:])\n",
    "            elif dataformat == 66:\n",
    "                # 32-bit delta encoded\n",
    "                data = self.decode32BitDelta(data[1:])\n",
    "            elif dataformat == 70:\n",
    "                # 16 to 8 bit conversion\n",
    "                data = self.decode16To8(data[1:])\n",
    "            elif dataformat == 71:\n",
    "                # 32 to 8 bit conversion\n",
    "                data = self.decode32To8(data[1:])\n",
    "            elif dataformat == 72:\n",
    "                # 'follow' encoding\n",
    "                data = self.followDecode(data[1:])\n",
    "            else:\n",
    "                # invalid/unsupported data format\n",
    "                raise ZTRDataFormatError(dataformat)\n",
    "\n",
    "            dataformat = unpack('b', data[0])[0]\n",
    "    \n",
    "        return (chtype, datalen, data)\n",
    "\n",
    "\n",
    "class ABIError(TraceFileError):\n",
    "    pass\n",
    "\n",
    "class ABIVersionError(ABIError):\n",
    "    def __init__(self, ver_major, ver_minor):\n",
    "        self.ver_major = ver_major\n",
    "        self.ver_minor = ver_minor\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'This file uses version ' + str(self.ver_major) + '.' + str(self.ver_minor) + ' of the ABI format.  This software only supports version 1.x of the format.'\n",
    "\n",
    "class ABIIndexError(ABIError):\n",
    "    def __init__(self, indexnum, indextotal):\n",
    "        self.indexnum = indexnum\n",
    "        self.indextotal = indextotal\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Error reading ABI file index entry ' + str(self.indexnum) + ' of ' + str(self.indextotal) + ' expected entries.  The file might be damaged.'\n",
    "\n",
    "class ABIDataError(ABIError):\n",
    "    def __init__(self, expectedlen, actuallen):\n",
    "        self.expectedlen = expectedlen\n",
    "        self.actuallen = actuallen\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Error reading ABI file data.  Expected ' + str(self.expectedlen) + ' bytes but only got ' + str(self.actuallen) + ' bytes.  The file appears to be damaged.'\n",
    "\n",
    "\n",
    "class ABISequenceTrace(SequenceTrace):\n",
    "    def loadFile(self, filename):\n",
    "        self.fname = filename\n",
    "\n",
    "        try:\n",
    "            self.tf = open(filename, 'rb')\n",
    "        except IOError:\n",
    "            raise\n",
    "\n",
    "        self.abiindex = list()\n",
    "\n",
    "        # read the ABI magic number\n",
    "        abinum = self.tf.read(4)\n",
    "        #print abinum\n",
    "        if abinum != 'ABIF':\n",
    "            raise ABIError('The ABI file header is invalid.  The file appears to be damaged.')\n",
    "\n",
    "        # check the major version number\n",
    "        try:\n",
    "            version = unpack('>H', self.tf.read(2))[0]\n",
    "        except struct.error:\n",
    "            raise ABIError('The ABI file header is invalid.  The file appears to be damaged.')\n",
    "        #print version\n",
    "        if (version / 100) != 1:\n",
    "            raise ABIVersionError(version / 100, version % 100)\n",
    "        \n",
    "        # skip the next 10 bytes\n",
    "        self.tf.read(10)\n",
    "        \n",
    "        # get the file index information\n",
    "        try:\n",
    "            index_entry_len = unpack('>h', self.tf.read(2))[0]\n",
    "            self.num_index_entries = unpack('>i', self.tf.read(4))[0]\n",
    "            total_index_size = unpack('>i', self.tf.read(4))[0]\n",
    "            self.index_offset = unpack ('>i', self.tf.read(4))[0]\n",
    "        except struct.error:\n",
    "            raise ABIError('The ABI file header is invalid.  The file appears to be damaged.')\n",
    "        \n",
    "        #print index_entry_len, self.num_index_entries, total_index_size, self.index_offset\n",
    "\n",
    "        self.readABIIndex()\n",
    "        \n",
    "        self.readBaseCalls()\n",
    "        self.readConfScores()\n",
    "        self.readTraceData()\n",
    "        self.readBaseLocations()\n",
    "        self.readComments()\n",
    "        \n",
    "    def readABIIndex(self):\n",
    "        # read the ABI index block\n",
    "        self.tf.seek(self.index_offset, 0)\n",
    "\n",
    "        for cnt in range(self.num_index_entries):\n",
    "            try:\n",
    "                self.abiindex.append(dict(did=0, idv=0, dformat=0, fsize=0, dcnt=0, dlen=0, offset=0))\n",
    "                self.abiindex[cnt]['did'] = self.tf.read(4)\n",
    "                self.abiindex[cnt]['idv'] = unpack('>I', self.tf.read(4))[0]\n",
    "                self.abiindex[cnt]['dformat'] = unpack('>H', self.tf.read(2))[0]\n",
    "                self.abiindex[cnt]['fsize'] = unpack('>H', self.tf.read(2))[0]\n",
    "                self.abiindex[cnt]['dcnt'] = unpack('>I', self.tf.read(4))[0]\n",
    "                self.abiindex[cnt]['dlen'] = unpack('>I', self.tf.read(4))[0]\n",
    "                self.abiindex[cnt]['offset'] = unpack('>I', self.tf.read(4))[0]\n",
    "                # skip 4 bytes (the unused \"data handle\" field)\n",
    "                self.tf.read(4)\n",
    "            except struct.error:\n",
    "                raise ABIIndexError(cnt, self.num_index_entries)\n",
    "        \n",
    "        #self.printABIIndex('CMNT')\n",
    "\n",
    "    def printABIIndex(self, data_id):\n",
    "        for entry in self.abiindex:\n",
    "            if entry['did'] == data_id:\n",
    "                print 'entry ID:', entry['did']\n",
    "                print 'idv:', entry['idv']\n",
    "                print 'data format:', entry['dformat']\n",
    "                print 'format size:', entry['fsize']\n",
    "                print 'data count:', entry['dcnt']\n",
    "                print 'total data length:', entry['dlen']\n",
    "                print 'data offset:', entry['offset']\n",
    "\n",
    "    def getIndexEntry(self, data_id, number):\n",
    "        for row in self.abiindex:\n",
    "            if (row['did'] == data_id) and (row['idv'] == number):\n",
    "                return row\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def getIndexEntriesById(self, data_id):\n",
    "        entries = list()\n",
    "    \n",
    "        for row in self.abiindex:\n",
    "            if row['did'] == data_id:\n",
    "                entries.append(row)\n",
    "    \n",
    "        return entries\n",
    "    \n",
    "    # Attempts to get a bunch of information about the sequencing run from the ABI file.  As much as possible,\n",
    "    # the keys used for individual comment values correspond with the keys used for the same values by the\n",
    "    # Staden software package.  However, this method also retrieves some comments that are not read by the\n",
    "    # Staden package.  To avoid confusion, these additional comment values are not given 4-letter keys.\n",
    "    def readComments(self):\n",
    "        # get the sample name\n",
    "        entry = self.getIndexEntry('SMPL', 1)\n",
    "        if entry:\n",
    "            self.comments['NAME'] = self.readString(entry)\n",
    "\n",
    "        # get the run name\n",
    "        entry = self.getIndexEntry('RunN', 1)\n",
    "        if entry:\n",
    "            self.comments['Run name'] = self.readString(entry)\n",
    "\n",
    "        # get the lane number\n",
    "        entry = self.getIndexEntry('LANE', 1)\n",
    "        if entry:\n",
    "            self.comments['LANE'] = str(self.read2ByteInts(entry)[0])\n",
    "\n",
    "        # get the signal strengths for each dye\n",
    "        entry = self.getIndexEntry('S/N%', 1)\n",
    "        if entry:\n",
    "            stvals = self.read2ByteInts(entry)\n",
    "\n",
    "            # use the \"filter wheel order\" to determine the base/value pairings\n",
    "            order = self.getBaseDataOrder()\n",
    "            sigst = {}\n",
    "            for cnt in range(0, len(order)):\n",
    "                sigst[order[cnt]] = stvals[cnt]\n",
    "\n",
    "            self.comments['SIGN'] = 'A={0},C={1},G={2},T={3}'.format(sigst['A'], sigst['C'], sigst['G'], sigst['T'])\n",
    "\n",
    "        # get the average peak spacing\n",
    "        entry = self.getIndexEntry('SPAC', 1)\n",
    "        if entry:\n",
    "            spacing = self.read4ByteFloats(entry)[0]\n",
    "            # if spacing is invalid, estimate it ourselves (the Staden code [seqIOABI.c] indicates this is a possibility)\n",
    "            if spacing < 0:\n",
    "                spacing = float(self.basepos[-1] - self.basepos[0]) / (len(self.basepos) - 1)\n",
    "            self.comments['SPAC'] = '{0:.2f}'.format(spacing)\n",
    "\n",
    "        # get the run dates and times\n",
    "        d_entries = self.getIndexEntriesById('RUND')\n",
    "        t_entries = self.getIndexEntriesById('RUNT')\n",
    "        if (len(d_entries) > 1) and (len(t_entries) > 1):\n",
    "            sdate = self.readDateTime(self.getIndexEntry('RUND', 1), self.getIndexEntry('RUNT', 1))\n",
    "            edate = self.readDateTime(self.getIndexEntry('RUND', 2), self.getIndexEntry('RUNT', 2))\n",
    "            #print sdate, edate\n",
    "            self.comments['RUND'] = sdate.strftime('%Y%m%d.%H%M%S') + ' - ' + edate.strftime('%Y%m%d.%H%M%S')\n",
    "            self.comments['DATE'] = sdate.strftime('%a %d %b %H:%M:%S %Y') + ' to ' + edate.strftime('%a %d %b %H:%M:%S %Y')\n",
    "\n",
    "        # get the data collection dates and times\n",
    "        if (len(d_entries) == 4) and (len(t_entries) == 4):\n",
    "            sdate = self.readDateTime(self.getIndexEntry('RUND', 3), self.getIndexEntry('RUNT', 3))\n",
    "            edate = self.readDateTime(self.getIndexEntry('RUND', 4), self.getIndexEntry('RUNT', 4))\n",
    "            #print sdate, edate\n",
    "            self.comments['Data coll. dates/times'] = sdate.strftime('%a %d %b %H:%M:%S %Y') + ' to ' + edate.strftime('%a %d %b %H:%M:%S %Y')\n",
    "\n",
    "        # get the dye set/primer (mobility) file\n",
    "        entry = self.getIndexEntry('PDMF', 1)\n",
    "        if entry:\n",
    "            self.comments['DYEP'] = self.readString(entry)\n",
    "\n",
    "        # get the sequencing machine name and serial number\n",
    "        entry = self.getIndexEntry('MCHN', 1)\n",
    "        if entry:\n",
    "            self.comments['MACH'] = self.readString(entry)\n",
    "\n",
    "        # get the sequencing machine model\n",
    "        entry = self.getIndexEntry('MODL', 1)\n",
    "        if entry:\n",
    "            self.comments['MODL'] = self.readString(entry)\n",
    "\n",
    "        # get the basecaller name\n",
    "        entry = self.getIndexEntry('SPAC', 2)\n",
    "        if entry:\n",
    "            self.comments['BCAL'] = self.readString(entry)\n",
    "\n",
    "        # get the data collection software version\n",
    "        entry = self.getIndexEntry('SVER', 1)\n",
    "        if entry:\n",
    "            self.comments['VER1'] = self.readString(entry)\n",
    "\n",
    "        # get the basecaller version\n",
    "        entry = self.getIndexEntry('SVER', 2)\n",
    "        if entry:\n",
    "            self.comments['VER2'] = self.readString(entry)\n",
    "\n",
    "        # get the plate size\n",
    "        entry = self.getIndexEntry('PSZE', 1)\n",
    "        if entry:\n",
    "            self.comments['Plate size'] = str(self.read4ByteInts(entry)[0])\n",
    "\n",
    "        # get the gel name\n",
    "        # This is included here because it is read by the Staden package, but it does not appear to be\n",
    "        # included in the modern ABIF documentation.\n",
    "        entry = self.getIndexEntry('GELN', 1)\n",
    "        if entry:\n",
    "            self.comments['GELN'] = self.readString(entry)\n",
    "\n",
    "        # get the instrument (matrix) file\n",
    "        # This is included here because it is read by the Staden package, but it does not appear to be\n",
    "        # included in the modern ABIF documentation.\n",
    "        entry = self.getIndexEntry('MTXF', 1)\n",
    "        if entry:\n",
    "            self.comments['MTXF'] = self.readString(entry)\n",
    "\n",
    "        # 'APrX' points to a long XML string with detailed information about the analysis protocol used\n",
    "        #entry = self.getIndexEntry('APrX', 1)\n",
    "        #if entry:\n",
    "        #    self.readString(entry)\n",
    "\n",
    "    def readDateTime(self, dateindexrow, timeindexrow):\n",
    "        # date format:\n",
    "        #   bits 31-16: year\n",
    "        #   bits 15-8: month\n",
    "        #   bits 7-0: day of month\n",
    "        # time format:\n",
    "        #   bits 31-24: hour\n",
    "        #   bits 23-16: minutes\n",
    "        #   bits 15-8: seconds\n",
    "        datenum = self.read4ByteInts(dateindexrow)[0]\n",
    "        timenum = self.read4ByteInts(timeindexrow)[0]\n",
    "        dateobj = datetime(year=(datenum >> 16), month=((datenum >> 8) & 0xff), day=(datenum & 0xff),\n",
    "                hour=(timenum >> 24), minute=((timenum >> 16) & 0xff), second=((timenum >> 8) & 0xff))\n",
    "\n",
    "        return dateobj\n",
    "\n",
    "    def readString(self, indexrow):\n",
    "        if indexrow['fsize'] != 1:\n",
    "            raise ABIError('Index entry contains an invalid format size for string data.')\n",
    "        if indexrow['dformat'] not in (2, 18, 19):\n",
    "            raise ABIError('Index entry contains an invalid data type for character data.')\n",
    "    \n",
    "        if indexrow['dlen'] <= 4:\n",
    "            # The actual data are stored in the offset field of the index entry.  Because the offset\n",
    "            # was read as an unsigned, big-endian integer, the bytes should be in the correct order for\n",
    "            # the following bit shift operations.\n",
    "            lst = list()\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                val = (indexrow['offset'] >> ((3 - cnt) * 8)) & 0xff\n",
    "                lst.append(chr(val))\n",
    "    \n",
    "            strval = ''.join(lst)\n",
    "        else:\n",
    "            # get the data from the file\n",
    "            self.tf.seek(indexrow['offset'], 0)\n",
    "            strval = self.tf.read(indexrow['dcnt'])\n",
    "    \n",
    "        if indexrow['dlen'] != len(strval):\n",
    "            raise ABIDataError(indexrow['dlen'], len(strval))\n",
    "\n",
    "        # If this is a Pascal-style string (format 18), then remove the first character (which specifies\n",
    "        # the string length).  If this is a C-style string (format 19), then remove the trailing\n",
    "        # null character.\n",
    "        if indexrow['dformat'] == 18:\n",
    "            strval = strval[1:]\n",
    "        elif indexrow['dformat'] == 19:\n",
    "            strval = strval[:-1]\n",
    "\n",
    "        return strval\n",
    "    \n",
    "    def read1ByteInts(self, indexrow):\n",
    "        if indexrow['fsize'] != 1:\n",
    "            raise ABIError('Index entry contains an invalid format size for 1-byte integers.')\n",
    "    \n",
    "        # see if the data format is signed or unsigned\n",
    "        if indexrow['dformat'] == 1:\n",
    "            formatstr = 'B'\n",
    "        elif indexrow['dformat'] == 2:\n",
    "            formatstr = 'b'\n",
    "        else:\n",
    "            raise ABIError('Index entry contains an invalid data type ID for 1-byte integers.')\n",
    "\n",
    "        lst = list()\n",
    "    \n",
    "        if indexrow['dlen'] <= 4:\n",
    "            # The actual data are stored in the offset field of the index entry.  Because the offset\n",
    "            # was read as an unsigned, big-endian integer, the bytes should be in the correct order for\n",
    "            # the following bit shift operations.\n",
    "            # First, repack the integer to deal with the possibility of signed integers (shift operations\n",
    "            # would only return positive values).\n",
    "            data = pack('>I', indexrow['offset'])\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                val = unpack(formatstr, data[cnt:cnt+1])[0]\n",
    "                lst.append(val)\n",
    "        else:\n",
    "            # get the data from the file\n",
    "            self.tf.seek(indexrow['offset'], 0)\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                lst.append(unpack(formatstr, self.tf.read(1))[0])\n",
    "    \n",
    "        if indexrow['dlen'] != len(lst):\n",
    "            raise ABIDataError(indexrow['dlen'], len(lst))\n",
    "\n",
    "        return lst\n",
    "    \n",
    "    def read2ByteInts(self, indexrow):\n",
    "        if indexrow['fsize'] != 2:\n",
    "            raise ABIError('Index entry contains an invalid format size for 2-byte integers.')\n",
    "\n",
    "        # see if the data format is signed or unsigned\n",
    "        if indexrow['dformat'] == 3:\n",
    "            formatstr = '>H'\n",
    "        elif indexrow['dformat'] == 4:\n",
    "            formatstr = '>h'\n",
    "        else:\n",
    "            raise ABIError('Index entry contains an invalid data type ID for 2-byte integers.')\n",
    "    \n",
    "        lst = list()\n",
    "    \n",
    "        if indexrow['dlen'] <= 4:\n",
    "            # The actual data are stored in the offset field of the index entry.  Because the offset\n",
    "            # was read as an unsigned, big-endian integer, the bytes should be in the correct order for\n",
    "            # the following operations.\n",
    "            # First, repack the integer to deal with the possibility of signed integers (shift operations\n",
    "            # would only return positive values).\n",
    "            data = pack('>I', indexrow['offset'])\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                val = unpack(formatstr, data[cnt*2:cnt*2+2])[0]\n",
    "                lst.append(val)\n",
    "        else:\n",
    "            # get the data from the file\n",
    "            self.tf.seek(indexrow['offset'], 0)\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                lst.append(unpack(formatstr, self.tf.read(2))[0])\n",
    "    \n",
    "        if indexrow['dlen'] != (len(lst) * 2):\n",
    "            raise ABIDataError(indexrow['dlen'], (len(lst) * 2))\n",
    "    \n",
    "        return lst\n",
    "    \n",
    "    def read4ByteInts(self, indexrow):\n",
    "        if indexrow['fsize'] != 4:\n",
    "            raise ABIError('Index entry contains an invalid format size for 4-byte integers.')\n",
    "        if indexrow['dformat'] not in (5, 10, 11):\n",
    "            raise ABIError('Index entry contains an invalid data type ID for 4-byte integers.')\n",
    "    \n",
    "        lst = list()\n",
    "    \n",
    "        if indexrow['dlen'] == 4:\n",
    "            # The actual data are stored in the offset field of the index entry.  In the case of 4-byte\n",
    "            # ints, the offset value is the data value.  It must be repacked, though, to reinterpret it\n",
    "            # as a signed integer.\n",
    "            data = pack('>I', indexrow['offset'])\n",
    "            val = unpack('>i', data)[0]\n",
    "            lst.append(val)\n",
    "        else:\n",
    "            # get the data from the file\n",
    "            self.tf.seek(indexrow['offset'], 0)\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                lst.append(unpack('>i', self.tf.read(4))[0])\n",
    "    \n",
    "        if indexrow['dlen'] != (len(lst) * 4):\n",
    "            raise ABIDataError(indexrow['dlen'], (len(lst) * 4))\n",
    "    \n",
    "        return lst\n",
    "\n",
    "    def read4ByteFloats(self, indexrow):\n",
    "        if indexrow['fsize'] != 4:\n",
    "            raise ABIError('Index entry contains an invalid format size for 4-byte floating point numbers.')\n",
    "        if indexrow['dformat'] != 7:\n",
    "            raise ABIError('Index entry contains an invalid data type ID for 4-byte floating point numbers.')\n",
    "    \n",
    "        lst = list()\n",
    "    \n",
    "        if indexrow['dlen'] <= 4:\n",
    "            # The actual data are stored in the offset field of the index entry.\n",
    "            data = pack('>I', indexrow['offset'])\n",
    "            lst.append(unpack('>f', data)[0])\n",
    "        else:\n",
    "            # get the data from the file\n",
    "            self.tf.seek(indexrow['offset'], 0)\n",
    "            for cnt in range(0, indexrow['dcnt']):\n",
    "                lst.append(unpack('>f', self.tf.read(4))[0])\n",
    "    \n",
    "        if indexrow['dlen'] != (len(lst) * 4):\n",
    "            raise ABIDataError(indexrow['dlen'], (len(lst) * 4))\n",
    "    \n",
    "        return lst\n",
    "\n",
    "    # According to the ABIF documentation, ABIF files (after base calling) should contain two base\n",
    "    # call entries (\"PBAS\"): one containing \"sequence characters edited by user\" (entry number 1),\n",
    "    # and one containing \"sequence characters as called by Basecaller\" (entry number 2).  These\n",
    "    # two entries will, in most cases, contain identical sequence data.  This method follows the same\n",
    "    # convention used by the Staden package (see seqIOABI.c), which is to only look at entry 1 (the\n",
    "    # user-edited sequence) and ignore entry 2.\n",
    "    def readBaseCalls(self):\n",
    "        row = self.getIndexEntry('PBAS', 1)\n",
    "        if row is None:\n",
    "            raise ABIError('No base call data were found in the ABI file.  The file might be damaged.')\n",
    "\n",
    "        # read the base calls from the file\n",
    "        self.basecalls = self.readString(row).upper()\n",
    "    \n",
    "    # There is an inconsistency in the ABIF file format documentation regarding the data format of the\n",
    "    # confidence scores.  The data format ID (as actually found in a .ab1 file) is 2, indicating the\n",
    "    # values are signed 1-byte integers.  The ABIF documentation, however, sugggests the values can range\n",
    "    # from 0-255 (i.e., an unsigned 1-byte integer).  In practice, the actual values do not appear to\n",
    "    # exceed 61, making the distinction between signed/unsigned irrelevant.  For now, the data format ID\n",
    "    # is taken as the correct indication of the underlying data format.\n",
    "    #\n",
    "    # According to the ABIF documentation, ABIF files (after base calling) should contain two quality\n",
    "    # value (QV) entries (\"PCON\"): one containing QVs \"as edited by user\" (entry number 1), and one\n",
    "    # containing QVs \"as called by Basecaller\" (entry number 2).  These two entries will, in most cases,\n",
    "    # contain identical values.  This method follows the same convention used by the Staden package\n",
    "    # (see seqIOABI.c), which is to only look at entry 1 (the user-edited QVs) and ignore entry 2.\n",
    "    def readConfScores(self):\n",
    "        row = self.getIndexEntry('PCON', 1)\n",
    "        if row is None:\n",
    "            raise ABIError('No confidence score data were found in the ABI file.  SeqTrace requires confidence scores for all base calls.')\n",
    "    \n",
    "        # read the base call confidence scores from the file\n",
    "        self.bcconf = self.read1ByteInts(row)\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    # According to the ABIF documentation, ABIF files (after base calling) should contain two peak\n",
    "    # location (PL) entries (\"PLOC\"): one containing PLs \"edited by user\" (entry number 1), and one\n",
    "    # containing PLs \"as called by Basecaller\" (entry number 2).  These two entries will, in most cases,\n",
    "    # contain identical information.  This method follows the same convention used by the Staden package\n",
    "    # (see seqIOABI.c), which is to only look at entry 1 (the user-edited PLs) and ignore entry 2.\n",
    "    def readBaseLocations(self):\n",
    "        row = self.getIndexEntry('PLOC', 1)\n",
    "        if row is None:\n",
    "            raise ABIError('No base location data were found in the ABI file.  The file might be damaged.')\n",
    "    \n",
    "        # read the base call locations from the file\n",
    "        self.basepos = self.read2ByteInts(row)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def getBaseDataOrder(self):\n",
    "        # retrieve the \"filter wheel order\" row from the file index\n",
    "        rows = self.getIndexEntriesById('FWO_')\n",
    "    \n",
    "        if len(rows) > 1:\n",
    "            raise ABIError('Found multiple filter wheel order index entries in ABI file.')\n",
    "        if rows[0]['dlen'] != 4:\n",
    "            raise ABIError('Incorrect data length for filter wheel order index entry.')\n",
    "    \n",
    "        # the data length is only 4 bytes, so the actual data is stored in the offset\n",
    "        val = rows[0]['offset']\n",
    "    \n",
    "        base_order = list()\n",
    "    \n",
    "        base_order.append(chr((val >> 24) & 0xff))\n",
    "        base_order.append(chr((val >> 16) & 0xff))\n",
    "        base_order.append(chr((val >> 8) & 0xff))\n",
    "        base_order.append(chr(val & 0xff))\n",
    "    \n",
    "        return base_order\n",
    "    \n",
    "    def readTraceData(self):\n",
    "        base_order = self.getBaseDataOrder()\n",
    "        maxval = 0\n",
    "        \n",
    "        # This is the ID for the first 'DATA' index entry that points to the processed\n",
    "        # trace data.  The man page for the Staden program convert_trace states\n",
    "        # that IDs 9-12 contain the processed data; IDs 1-4 contain the raw data.\n",
    "        # The ABIF documentation from ABI also suggests that IDs 1-8 will always contain\n",
    "        # raw data, and 9-12 will contain the processed data.  Is this always correct?\n",
    "        start_id = 9\n",
    "    \n",
    "        for cnt in range(0, 4):\n",
    "            row = self.getIndexEntry('DATA', start_id + cnt)\n",
    "            if row == None:\n",
    "                raise ABIError('Could not find trace data index entries for all bases.  The file might be damaged.')\n",
    "    \n",
    "            # read the trace data from the file\n",
    "            lst = self.read2ByteInts(row)\n",
    "            tmpmax = max(lst)\n",
    "            if tmpmax > maxval:\n",
    "                maxval = tmpmax\n",
    "            self.tracesamps[base_order[cnt]] = lst\n",
    "\n",
    "        self.max_traceval = maxval\n",
    "\n",
    "\n",
    "class SCFError(TraceFileError):\n",
    "    pass\n",
    "\n",
    "class SCFVersionError(SCFError):\n",
    "    def __init__(self, version, revision):\n",
    "        self.version = version\n",
    "        self.revision = revision\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'This file uses version ' + self.version + '.' + self.revision + ' of the SCF format.  This software only supports version 3.00 of the format.'\n",
    "\n",
    "class SCFDataError(SCFError):\n",
    "    def __init__(self, expectedlen, actuallen):\n",
    "        self.expectedlen = expectedlen\n",
    "        self.actuallen = actuallen\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Error reading SCF file data.  Expected ' + str(self.expectedlen) + ' bytes but only got ' + str(self.actuallen) + ' bytes.  The file appears to be damaged.'\n",
    "\n",
    "\n",
    "class SCFSequenceTrace(SequenceTrace):\n",
    "    def loadFile(self, filename):\n",
    "        self.fname = filename\n",
    "\n",
    "        try:\n",
    "            self.tf = open(filename, 'rb')\n",
    "        except IOError:\n",
    "            raise\n",
    "\n",
    "        magicnum = self.tf.read(4)\n",
    "        #print magicnum\n",
    "        if magicnum != '.scf':\n",
    "            raise SCFError('The SCF file header is invalid.  The file appears to be damaged.')\n",
    "\n",
    "        try:\n",
    "            numsamps = unpack('>I', self.tf.read(4))[0]\n",
    "            sampstart = unpack('>I', self.tf.read(4))[0]\n",
    "            #print numsamps, sampstart\n",
    "\n",
    "            numbases = unpack('>I', self.tf.read(4))[0]\n",
    "            # skip 8 bytes\n",
    "            self.tf.read(8)\n",
    "            basesstart = unpack('>I', self.tf.read(4))[0]\n",
    "            #print numbases, basesstart\n",
    "\n",
    "            commentslen = unpack('>I', self.tf.read(4))[0]\n",
    "            commentsstart = unpack('>I', self.tf.read(4))[0]\n",
    "            #print commentslen, commentsstart\n",
    "\n",
    "            version = self.tf.read(4)\n",
    "            #print version\n",
    "\n",
    "            samplesize = unpack('>I', self.tf.read(4))[0]\n",
    "            codeset = unpack('>I', self.tf.read(4))[0]\n",
    "            #print samplesize, codeset\n",
    "        except struct.error:\n",
    "            raise SCFError('The SCF file header is invalid.  The file appears to be damaged.')\n",
    "\n",
    "        if version != '3.00':\n",
    "            raise SCFVersionError(version[0], version[2:])\n",
    "\n",
    "        if samplesize not in (1, 2):\n",
    "            raise SCFError('Invalid sample size value in SCF header.  The size specified was ' + str(samplesize) + ', but must be either 1 or 2.')\n",
    "\n",
    "        if codeset != 0:\n",
    "            raise SCFError('Invalid code set specified in SCF header.  This file uses code set ' + str(codeset) + ', but this software only supports code set 0.')\n",
    "\n",
    "        self.readBasesData(numbases, basesstart)\n",
    "        self.readTraceData(numsamps, sampstart, samplesize)\n",
    "        self.readComments(commentslen, commentsstart)\n",
    "\n",
    "    def readBasesData(self, numbases, basesstart):\n",
    "        self.basepos = list()\n",
    "        probs = {'A': list(), 'C': list(), 'G': list(), 'T': list()}\n",
    "        self.bcconf = list()\n",
    "\n",
    "        self.tf.seek(basesstart, 0)\n",
    "\n",
    "        try:\n",
    "            # get the base locations\n",
    "            for cnt in range(0, numbases):\n",
    "                index = unpack('>I', self.tf.read(4))[0]\n",
    "                self.basepos.append(index)\n",
    "            #print self.basepos\n",
    "\n",
    "            # get the base call probabilities for all bases\n",
    "            for base in ('A', 'C', 'G', 'T'):\n",
    "                for cnt in range(0, numbases):\n",
    "                    prob = unpack('B', self.tf.read(1))[0]\n",
    "                    probs[base].append(prob)\n",
    "        except struct.error:\n",
    "            raise SCFError('Error while reading base call locations and probabilities from the SCF file.  The file appears to be damaged.')\n",
    "\n",
    "        # get the base calls\n",
    "        self.basecalls = self.tf.read(numbases).upper()\n",
    "        #print self.basecalls\n",
    "        if numbases != len(self.basecalls):\n",
    "            raise SCFDataError(numbases, len(self.basecalls))\n",
    "\n",
    "        # build the confidence scores list\n",
    "        for cnt in range(0, numbases):\n",
    "            base = self.basecalls[cnt]\n",
    "            self.bcconf.append(probs[base][cnt])\n",
    "\n",
    "        #print self.bcconf\n",
    "\n",
    "    def readTraceData(self, numsamps, sampstart, sampsize):\n",
    "        if sampsize == 1:\n",
    "            formatstr = 'B'\n",
    "        else:\n",
    "            formatstr = '>H'\n",
    "\n",
    "        self.tf.seek(sampstart, 0)\n",
    "\n",
    "        maxval = 0\n",
    "\n",
    "        for base in ('A', 'C', 'G', 'T'):\n",
    "            samps = list()\n",
    "\n",
    "            try:\n",
    "                # read the raw sample data\n",
    "                for cnt in range(0, numsamps):\n",
    "                    val = unpack(formatstr, self.tf.read(sampsize))[0]\n",
    "                    samps.append(val)\n",
    "            except struct.error:\n",
    "                raise SCFDataError((numsamps * sampsize), (len(samps) * sampsize))\n",
    "\n",
    "            # sample values are double-delta encoded (i.e., two successive rounds of differences)\n",
    "            if sampsize == 1:\n",
    "                self.decode8BitDoubleDelta(samps)\n",
    "            else:\n",
    "                self.decode16BitDoubleDelta(samps)\n",
    "\n",
    "            self.tracesamps[base] = samps\n",
    "            tmpmax = max(self.tracesamps[base])\n",
    "            if tmpmax > maxval:\n",
    "                maxval = tmpmax\n",
    "\n",
    "        self.max_traceval = maxval\n",
    "        #print self.tracesamps['A']\n",
    "\n",
    "    def decode8BitDoubleDelta(self, data):\n",
    "        for clev in range(0, 2):\n",
    "            prev = 0\n",
    "            for cnt in range(0, len(data)):\n",
    "                actual = data[cnt] + prev\n",
    "\n",
    "                # simulate 1-byte integer overflow, if needed\n",
    "                if actual > 255:\n",
    "                    actual -= 256\n",
    "\n",
    "                prev = actual\n",
    "                data[cnt] = actual\n",
    "    \n",
    "    def decode16BitDoubleDelta(self, data):\n",
    "        for clev in range(0, 2):\n",
    "            prev = 0\n",
    "            for cnt in range(0, len(data)):\n",
    "                actual = data[cnt] + prev\n",
    "\n",
    "                # simulate 2-byte integer overflow, if needed\n",
    "                if actual > 65535:\n",
    "                    actual -= 65536\n",
    "\n",
    "                prev = actual\n",
    "                data[cnt] = actual\n",
    "\n",
    "    def readComments(self, commentslen, commentsstart):\n",
    "        self.tf.seek(commentsstart, 0)\n",
    "\n",
    "        total = 0\n",
    "        while total < (commentslen - 1):\n",
    "            line = self.tf.readline()\n",
    "            total += len(line)\n",
    "\n",
    "            if line == '':\n",
    "                raise SCFError('Unable to read the comments section of the SCF file.  The file appears to be damaged.')\n",
    "\n",
    "            # get rid of the trailing '\\n'\n",
    "            line = line[:-1]\n",
    "\n",
    "            key, sep, value = line.partition('=')\n",
    "            #print key + ': ' + value\n",
    "            self.comments[key] = value\n",
    "\n",
    "        # make sure the next character is the null-terminator for the comments list\n",
    "        if self.tf.read(1) != '\\0':\n",
    "            raise SCFError('Missing null character at end of comments section.  The file appears to be damaged.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    #st = SCFSequenceTrace()\n",
    "    #st.loadFile('./sequencings/test2.scf')\n",
    "\n",
    "    #st = ZTRSequenceTrace()\n",
    "    #st.loadFile('forward.ztr')\n",
    "\n",
    "    #st = ABISequenceTrace()\n",
    "    #st.loadFile('forward.ab1')\n",
    "\n",
    "    #for key, value in sorted(st.getComments().iteritems()):\n",
    "    #    print key + ': ' + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stf = SequenceTraceFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = stf.loadTraceFile(\"./sequencings/test2.scf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTTTGTGCTGCGCCGACAATGAGATTTCCTTCAATTTTTACTGCAGTTTTATTCGCAGCATCCTCCGCATTAGCTGCTCCAGTCAACACTACAACAGAAGATGAAACGGCACAAATTCCGGCTGAAGCTGTCATCGGTTACTTAGATTTAGAAGGGGATTTCGATGTTGCTGTTTTGCCATTTTCCAACAGCACAAATAACGGGTTATTGTTTATAAATACTACTATTGCCAGCATTGCTGCTAAAGAAGAAGGGGTATCTTTGGATAAAAGAGAGGCTGAAGCTCAGCTGGCATTTAATAATAACCCATCGAGTGTAGGCGCCTACAGTTCAGGGACATACCGTAACCTCGCACAAGAAATGGGTAAAACAAATATACAGCAAAAGGTGAATAGTACTTTTGACAATATGTTTGGCTATAACAACACACAACAACTTTACTACCCGTACACCGAAAACGGTGTTTATAAAGCACACTACATAAAAGCAATTAACCCAGACGAAGGCGACGATATAAGAACAGAAGGGCAATCGTGGGGAATGACCGCCGCTGTCATGCTTAATAAACAAGAAGAATTTGATAACCTATGGCGCTTTGCAAAGGCGTATCAAAAAAATCCAGACAATCACCCTGATGCTAAAAAACAAGGCGTTTACGCGTGGAAACTAAAGCTTAATCAAAACGGCTTTGTTTATAAAGTGGATGAGGGCCCCGCTCCCGATGGCGAAGAGTACTTTGCGTTTGCACTACTTAATGCCTCTGCTCGTTGGGGGAATTCGGGTGAGTTTAACTACTACAACGATGCCATTACCATGTTAAACACAATTAAAAATAAGCTGATGGAAAACCAAATAATCCGCTTTTCACCTTACATTGATAACCTAACAGACCCTTCTTACCATATACCTGCGTTTTACGACTACTTTGCAAATAACGTAACTAACCAAGCAGACAAAATTACTGGCGACAGTAGCCACAAAAGTAGACCTTACTTAAAACCATTTTACAAAGTAAGTGGTAGCCCGATTGGAATTACCTACTTTTTATCCGCTTAAATGGCAGCCTGGTATTGGCTACTTTTTACCGCCAGCAACCCCGGTCATGGGTGGATTTTGGCTGGCGCGAAATTTAATAGGGGTTAAACCCATTTAAGGGGCGCACGCGGGGAAAAAGCAAAAAAAACACGCGGTTATTTTCGACACAAAACAACACATGCTTGGCGTTTCTCTCGGCGGAGCACGGAGACGAAGGGCGCCAACAAACGGATATTTTTGGGGGAGGTTGTGTATTTGTGGAGGGAGAGTTTTGATTTTGTTTTTGTTTCCAGGTGTTCAT'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.basecalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sequencings/test2.scf\n"
     ]
    }
   ],
   "source": [
    "print s.fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.basecalls == \"\".join('''TTTTGTGCTGCGCCGACAATGAGATTTCCTTCAATTTTTACTGCAGTTTTATTCGCAGCA\n",
    "TCCTCCGCATTAGCTGCTCCAGTCAACACTACAACAGAAGATGAAACGGCACAAATTCCG\n",
    "GCTGAAGCTGTCATCGGTTACTTAGATTTAGAAGGGGATTTCGATGTTGCTGTTTTGCCA\n",
    "TTTTCCAACAGCACAAATAACGGGTTATTGTTTATAAATACTACTATTGCCAGCATTGCT\n",
    "GCTAAAGAAGAAGGGGTATCTTTGGATAAAAGAGAGGCTGAAGCTCAGCTGGCATTTAAT\n",
    "AATAACCCATCGAGTGTAGGCGCCTACAGTTCAGGGACATACCGTAACCTCGCACAAGAA\n",
    "ATGGGTAAAACAAATATACAGCAAAAGGTGAATAGTACTTTTGACAATATGTTTGGCTAT\n",
    "AACAACACACAACAACTTTACTACCCGTACACCGAAAACGGTGTTTATAAAGCACACTAC\n",
    "ATAAAAGCAATTAACCCAGACGAAGGCGACGATATAAGAACAGAAGGGCAATCGTGGGGA\n",
    "ATGACCGCCGCTGTCATGCTTAATAAACAAGAAGAATTTGATAACCTATGGCGCTTTGCA\n",
    "AAGGCGTATCAAAAAAATCCAGACAATCACCCTGATGCTAAAAAACAAGGCGTTTACGCG\n",
    "TGGAAACTAAAGCTTAATCAAAACGGCTTTGTTTATAAAGTGGATGAGGGCCCCGCTCCC\n",
    "GATGGCGAAGAGTACTTTGCGTTTGCACTACTTAATGCCTCTGCTCGTTGGGGGAATTCG\n",
    "GGTGAGTTTAACTACTACAACGATGCCATTACCATGTTAAACACAATTAAAAATAAGCTG\n",
    "ATGGAAAACCAAATAATCCGCTTTTCACCTTACATTGATAACCTAACAGACCCTTCTTAC\n",
    "CATATACCTGCGTTTTACGACTACTTTGCAAATAACGTAACTAACCAAGCAGACAAAATT\n",
    "ACTGGCGACAGTAGCCACAAAAGTAGACCTTACTTAAAACCATTTTACAAAGTAAGTGGT\n",
    "AGCCCGATTGGAATTACCTACTTTTTATCCGCTTAAATGGCAGCCTGGTATTGGCTACTT\n",
    "TTTACCGCCAGCAACCCCGGTCATGGGTGGATTTTGGCTGGCGCGAAATTTAATAGGGGT\n",
    "TAAACCCATTTAAGGGGCGCACGCGGGGAAAAAGCAAAAAAAACACGCGGTTATTTTCGA\n",
    "CACAAAACAACACATGCTTGGCGTTTCTCTCGGCGGAGCACGGAGACGAAGGGCGCCAAC\n",
    "AAACGGATATTTTTGGGGGAGGTTGTGTATTTGTGGAGGGAGAGTTTTGATTTTGTTTTT\n",
    "GTTTCCAGGTGTTCAT'''.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test2.scf'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.getFileName()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# http://biopython.org/DIST/docs/api/Bio.File-pysrc.html#as_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pydna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "a = \"catcatgcgtcgtctactattcgatcttcgactat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "b= \"cgtcgtctactatncgatctt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "matches = pydna.common_sub_strings(a,b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0, 13), (21, 14, 7), (15, 8, 5), (30, 8, 5)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "newmatches = [matches[0],]\n",
    "for i, x in enumerate(matches[1:]):\n",
    "    g,f,h = matches[i]\n",
    "    if g+h < x[0] and f+h < x[1]:\n",
    "        newmatches.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0, 13), (21, 14, 7)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Bio.SeqFeature import SeqFeature, FeatureLocation, CompoundLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for m in newmatches:\n",
    "    l.append(FeatureLocation(m[0],m[0]+m[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cl = CompoundLocation(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x=pydna.Dseqrecord(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x.features = [SeqFeature(cl, type=\"trace\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9f60112991a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'ape x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bjorn/anaconda/envs/bjorn/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2203\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2204\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2205\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2207\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bjorn/anaconda/envs/bjorn/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2124\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2125\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2126\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2127\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mape\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32m/home/bjorn/anaconda/envs/bjorn/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bjorn/.ipython/profile_default/startup/00_ape.py\u001b[0m in \u001b[0;36mape\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m   \u001b[1;31m# new\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mMyMagics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apeloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#(*args,**kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'description'"
     ]
    }
   ],
   "source": [
    "ape x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pydna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = pydna.read(\"/home/bjorn/Desktop/pydna-DNA-assembly/constructs/Leila/pYPK0_RPS19atp_MFalpha_PhXYL_RPL12atp.gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s.map_target = slice(1140, 1140 + 330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A3_premix.scf', 'A1_premix.scf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.map_trace_files(\"/home/bjorn/Desktop/pydna-DNA-assembly/constructs/Leila/Leila sequencings/*.scf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOCUS       .                       8513 bp    DNA     circular UNK 09-FEB-2015\n",
       "DEFINITION  \n",
       "ACCESSION   <unknown id>\n",
       "VERSION     <unknown id>\n",
       "DBLINK      BioProject:PRJNA43747\n",
       "KEYWORDS    .\n",
       "SOURCE      \n",
       "  ORGANISM  . .\n",
       "            .\n",
       "FEATURES             Location/Qualifiers\n",
       "     overlap         363..486\n",
       "                     /note=\"olp_GqXRnFdj0bUssSD9FyJPEMLnbh0\"\n",
       "                     /ApEinfo_revcolor=\"#EACBDB\"\n",
       "                     /chksum=\"GqXRnFdj0bUssSD9FyJPEMLnbh0\"\n",
       "                     /ApEinfo_fwdcolor=\"#FEC4FC\"\n",
       "     misc            531..556\n",
       "                     /label=\"652_ScRPS19atpf2\"\n",
       "     promoter        537..1080\n",
       "                     /note=\"tp YOL120C YOL121C\"\n",
       "                     /ApEinfo_fwdcolor=\"#b1e6cc\"\n",
       "                     /ApEinfo_revcolor=\"#b1e681\"\n",
       "     primer_bind     537..556\n",
       "                     /note=\"652_ScRPS19atpf2\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     primer_bind     complement(1061..1080)\n",
       "                     /note=\"651_ScRPS19atpr_PacI\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     misc            complement(1061..1087)\n",
       "                     /label=\"651_ScRPS19atpr_PacI\"\n",
       "     overlap         1088..1137\n",
       "                     /note=\"olp_8ro7GZnjXs9RDyEisndJYyFeLIU\"\n",
       "                     /ApEinfo_revcolor=\"#BCBBEF\"\n",
       "                     /chksum=\"8ro7GZnjXs9RDyEisndJYyFeLIU\"\n",
       "                     /ApEinfo_fwdcolor=\"#D7DDD2\"\n",
       "     primer_bind     complement(1115..1135)\n",
       "                     /note=\"567_pCAPsAjiIF\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     misc            complement(1115..1137)\n",
       "                     /label=\"567_pCAPsAjiIF\"\n",
       "     primer_bind     complement(1115..1137)\n",
       "                     /note=\"567_pCAPsAjiIF\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "     misc            complement(1115..1137)\n",
       "                     /label=\"567_pCAPsAjiIF\"\n",
       "     misc            1138..1169\n",
       "                     /label=\"Fw_alpha\"\n",
       "     primer_bind     1140..1169\n",
       "                     /note=\"Fw_alpha\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     overlap         1371..1406\n",
       "                     /note=\"olp_stADXvkrdLBZ/DDbKucgMkmk4po\"\n",
       "                     /ApEinfo_revcolor=\"#E6E8E6\"\n",
       "                     /chksum=\"stADXvkrdLBZ/DDbKucgMkmk4po\"\n",
       "                     /ApEinfo_fwdcolor=\"#BDCDDA\"\n",
       "     primer_bind     complement(1377..1406)\n",
       "                     /note=\"Rev_alpha\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     misc            complement(1377..1406)\n",
       "                     /label=\"Rev_alpha\"\n",
       "     primer_bind     complement(1377..1406)\n",
       "                     /note=\"Rev_alpha\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "     misc            complement(1377..1406)\n",
       "                     /label=\"Rev_alpha\"\n",
       "     misc_feature    1413..2630\n",
       "                     /ApEinfo_graphicformat=\"arrow_data {{0 1 2 0 0 -1} {} 0}\n",
       "                     width 5 offset 0\"\n",
       "                     /ApEinfo_fwdcolor=\"#d0ff76\"\n",
       "                     /label=\"xyl coding sequence\"\n",
       "                     /ApEinfo_revcolor=\"green\"\n",
       "     primer_bind     1413..1432\n",
       "                     /note=\"PhXYLf_New\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     primer_bind     complement(2608..2630)\n",
       "                     /note=\"PhXYLr_ZraI\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     misc            complement(2608..2667)\n",
       "                     /label=\"PhXYLr_ZraI\"\n",
       "     overlap         2637..2667\n",
       "                     /note=\"olp_ucNuXLfr2NLE3QLKTobL1QWrl64\"\n",
       "                     /ApEinfo_revcolor=\"#BCC2ED\"\n",
       "                     /chksum=\"ucNuXLfr2NLE3QLKTobL1QWrl64\"\n",
       "                     /ApEinfo_fwdcolor=\"#DAD8D9\"\n",
       "     misc            2668..2695\n",
       "                     /label=\"751_RPL12A_fw\"\n",
       "     primer_bind     2674..2695\n",
       "                     /note=\"751_RPL12A_fw\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     primer_bind     complement(3334..3355)\n",
       "                     /note=\"752_RPL12A_rv\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "     misc            complement(3334..3362)\n",
       "                     /label=\"752_RPL12A_rv\"\n",
       "     overlap         3416..3657\n",
       "                     /note=\"olp_WozPxSoJjWK29v8yQeT4MATRraA\"\n",
       "                     /ApEinfo_revcolor=\"#E0E2DD\"\n",
       "                     /chksum=\"WozPxSoJjWK29v8yQeT4MATRraA\"\n",
       "                     /ApEinfo_fwdcolor=\"#CDD2BA\"\n",
       "     primer_bind     complement(3629..3657)\n",
       "                     /note=\"578_crp42-70\"\n",
       "                     /ApEinfo_fwdcolor=\"green\"\n",
       "                     /ApEinfo_revcolor=\"red\"\n",
       "     misc            complement(3629..3657)\n",
       "                     /label=\"578_crp42-70\"\n",
       "     rep_origin      complement(4046..4728)\n",
       "                     /ApEinfo_graphicformat=\"arrow_data {{0 1 2 0 0 -1} {} 0}\n",
       "                     width 5 offset 0\"\n",
       "                     /ApEinfo_revcolor=\"gray50\"\n",
       "                     /label=\"ColE1 origin\"\n",
       "                     /ApEinfo_fwdcolor=\"gray50\"\n",
       "     misc_feature    4730..5788\n",
       "                     /ApEinfo_graphicformat=\"arrow_data {{0 1 2 0 0 -1} {} 0}\n",
       "                     width 5 offset 0\"\n",
       "                     /ApEinfo_revcolor=\"green\"\n",
       "                     /label=\"URA3\"\n",
       "                     /ApEinfo_fwdcolor=\"cyan\"\n",
       "     CDS             complement(7460..8119)\n",
       "                     /ApEinfo_graphicformat=\"arrow_data {{0 1 2 0 0 -1} {} 0}\n",
       "                     width 5 offset 0\"\n",
       "                     /ApEinfo_revcolor=\"yellow\"\n",
       "                     /label=\"AmpR\"\n",
       "                     /ApEinfo_fwdcolor=\"yellow\"\n",
       "     trace           1139..2079\n",
       "                     /label=\"A3_premix.scf\"\n",
       "     trace           join(1139..1723,1725..2079)\n",
       "                     /label=\"A1_premix.scf\"\n",
       "ORIGIN\n",
       "        1 tcgcgcgttt cggtgatgac ggtgaaaacc tctgacacat gcagctcccg gagacggtca\n",
       "       61 cagcttgtct gtaagcggat gccgggagca gacaagcccg tcagggcgcg tcagcgggtg\n",
       "      121 ttggcgggtg tcggggctgg cttaactatg cggcatcaga gcagattgta ctgagagtgc\n",
       "      181 accatagatc ctgaggatcg gggtgataaa tcagtctgcg ccacatcggg ggaaacaaaa\n",
       "      241 tggcgcgaga tctaaaaaaa aaggctccaa aaggagcctt tcgcgctacc aggtaacgcg\n",
       "      301 ccactccgac gggattaacg agtgccgtaa acgacgatgg ttttaccgtg tgcggagatc\n",
       "      361 aggttctgat cctcgagcat cttaagaatt cgtcccacgg tttgtctaga gcagccgaca\n",
       "      421 atctggccaa tttcctgacg ggtaattttg atttgcatgc cgtccgggtg agtcatagcg\n",
       "      481 tctggttgtt ttgccagatt cagcagagtc tgtgcaatgc ggccgctgac ttaaatgtta\n",
       "      541 actgaaatga aaatttcata tttacttttt tattgttact catttgtaat tcataaacta\n",
       "      601 catacacttt caatcgtttc tttcaaacta cataattttt actggcgatc aataacgcat\n",
       "      661 ttagttcata aagtgagtca aagttaacac tagaatattt gctacaccat caataggcta\n",
       "      721 gaccatagtt gaaaactttc ataataaatt cttccgtttt caatcttcat atactgtgtc\n",
       "      781 tctaaccatg ataccgtgac acaactacat ccgtacacat gtgacgttcg ttcaacccgt\n",
       "      841 acatttatat aaaaccgttc tggcggcctt ttattttttt acatttctta tgatcgggat\n",
       "      901 tgcagaacgc cgtgaaattt ttcaatgtga ggttcggcct tgtttgcaaa agccctattg\n",
       "      961 agataccgga aagatatagg tgaaatgaag aaaactatgg gttgtatatc taataccccg\n",
       "     1021 gtgcgtttat taatatttta gcttgaaagc gaagtgatac gatcgacaaa tagagtaaaa\n",
       "     1081 ttaattagtc gaggaacgcc aggttgccca ctttctcact agtgacctgc agccgacaaa\n",
       "     1141 tgagatttcc ttcaattttt actgcagttt tattcgcagc atcctccgca ttagctgctc\n",
       "     1201 cagtcaacac tacaacagaa gatgaaacgg cacaaattcc ggctgaagct gtcatcggtt\n",
       "     1261 acttagattt agaaggggat ttcgatgttg ctgttttgcc attttccaac agcacaaata\n",
       "     1321 acgggttatt gtttataaat actactattg ccagcattgc tgctaaagaa gaaggggtat\n",
       "     1381 ctttggataa aagagaggct gaagctcagc tggcatttaa taataaccca tcgagtgtag\n",
       "     1441 gcgcctacag ttcagggaca taccgtaacc tcgcacaaga aatgggtaaa acaaatatac\n",
       "     1501 agcaaaaggt gaatagtact tttgacaata tgtttggcta taacaacaca caacaacttt\n",
       "     1561 actacccgta caccgaaaac ggtgtttata aagcacacta cataaaagca attaacccag\n",
       "     1621 acgaaggcga cgatataaga acagaagggc aatcgtgggg aatgaccgcc gctgtcatgc\n",
       "     1681 ttaataaaca agaagaattt gataacctat ggcgctttgc aaaagcgtat caaaaaaatc\n",
       "     1741 cagacaatca ccctgatgct aaaaaacaag gcgtttacgc gtggaaacta aagcttaatc\n",
       "     1801 aaaacggctt tgtttataaa gtggatgagg gccccgctcc cgatggcgaa gagtactttg\n",
       "     1861 cgtttgcact acttaatgcc tctgctcgtt gggggaattc gggtgagttt aactactaca\n",
       "     1921 acgatgccat taccatgtta aacacaatta aaaataagct gatggaaaac caaataatcc\n",
       "     1981 gcttttcacc ttacattgat aacctaacag acccttctta ccatatacct gcgttttacg\n",
       "     2041 actactttgc aaataacgta actaaccaag cagacaaaaa ttactggcga caagtagcca\n",
       "     2101 caaaaagtag aaccttactt aaaaaccatt ttacaaaagt aagtggtagc ccgcattgga\n",
       "     2161 acttacctac atttttatcg cgcttagatg gcagccctgt tattggctac atttttaacg\n",
       "     2221 gccaagcaaa cccaggtcaa tggtatgaat ttgatgcatg gcgcgtaatt atgaatgtgg\n",
       "     2281 gtttagacgc gcatttaatg ggtgctcaag cgtggcataa aagtgcagtt aataaagcac\n",
       "     2341 tgggcttttt aagttatgca aaaacaaaca acagtaaaaa ctgttacgag caagtgtatt\n",
       "     2401 cgtacggtgg agcgcaaaac agaggctgtg caggcgaagg tcaaaaagcc gcgaatgcag\n",
       "     2461 tagcgttact tgcttcaaca aatgctgggc aagcaaatga gttttttaac gaattttggt\n",
       "     2521 ctttatcgca accaacgggt gactaccgtt actataatgg ttcgttatat atgttagcta\n",
       "     2581 tgctgcatgt atcgggcaat tttaagtttt ataacaacac gtttaattaa gacgtcgtgc\n",
       "     2641 catctgtgca gacaaacgca tcaggattta aattctggaa tttaccctaa ctgtacaaaa\n",
       "     2701 tatatttagc attatatacc cgctatttaa ttattataaa ggagaccttt atactcaaac\n",
       "     2761 aaatgaagcc accttatgcc attatcccat tcgtaccagt ctaaaacaag ataaataccc\n",
       "     2821 tgtttaacat ccgaacacca aatatgtgtg ggtgtcaaac aaagagacaa tgtaatgaga\n",
       "     2881 caatgtactg cttgtaaact ataacagtat cagctcaatt tcaatgccat ctactccaga\n",
       "     2941 tatcgttgcg attgtaccta caagccgcct acaacggcgt tattgttgta agctggtaaa\n",
       "     3001 atgcagtgcg ttcccccgtt gtgagttcgg gccaaactta cttggctggg agttccgtcc\n",
       "     3061 gtacagttgt ggcttgctgc ctagacttag gcggacgcgg cggcaattac tcctgggctc\n",
       "     3121 tccctagagc tggagagaca ctcttggtaa ccgaagggaa aatcccggaa gaaaatttgg\n",
       "     3181 aatccgttaa cactgctgga aatggacatc tcctaatact gaaaaaatta ttcaaagtct\n",
       "     3241 agaattttgg taaatttcaa ggcttctttg agttacagtt cgttactatt acaccttaaa\n",
       "     3301 ggaactttct tttggggtct agaaagtcga caacaagaac aaaggatata caaaattaat\n",
       "     3361 taatccggat ttacctgaat caattggcga aattttttgt acgaaatttc agccacttca\n",
       "     3421 caggcggttt tcgcacgtac ccatgcgcta cgttcctggc cctcttcaaa caggcccagt\n",
       "     3481 tcgccaataa aatcaccctg attcagatag gagaggatca tttctttacc ctcttcgtct\n",
       "     3541 ttgatcagca ctgccacaga gcctttaacg atgtagtaca gcgtttccgc tttttcaccc\n",
       "     3601 tggtgaataa gcgtgctctt ggatgggtac ttatgaatgt ggcaatgaga caagaaccat\n",
       "     3661 tcgagagtag gatccgtttg aggtttacca agtaccataa gatccttaaa tttttattat\n",
       "     3721 ctagctagat gataatatta tatcaagaat tgtacctgaa agcaaataaa ttttttatct\n",
       "     3781 ggcttaacta tgcggcatca gagcagattg tactgagagt gcaccatatg cggtgtgaaa\n",
       "     3841 taccgcacag atgcgtaagg agaaaatacc gcatcaggcg ctcttccgct tcctcgctca\n",
       "     3901 ctgactcgct gcgctcggtc gttcggctgc ggcgagcggt atcagctcac tcaaaggcgg\n",
       "     3961 taatacggtt atccacagaa tcaggggata acgcaggaaa gaacatgtga gcaaaaggcc\n",
       "     4021 agcaaaaggc caggaaccgt aaaaaggccg cgttgctggc gtttttccat aggctccgcc\n",
       "     4081 cccctgacga gcatcacaaa aatcgacgct caagtcagag gtggcgaaac ccgacaggac\n",
       "     4141 tataaagata ccaggcgttt ccccctggaa gctccctcgt gcgctctcct gttccgaccc\n",
       "     4201 tgccgcttac cggatacctg tccgcctttc tcccttcggg aagcgtggcg ctttctcata\n",
       "     4261 gctcacgctg taggtatctc agttcggtgt aggtcgttcg ctccaagctg ggctgtgtgc\n",
       "     4321 acgaaccccc cgttcagccc gaccgctgcg ccttatccgg taactatcgt cttgagtcca\n",
       "     4381 acccggtaag acacgactta tcgccactgg cagcagccac tggtaacagg attagcagag\n",
       "     4441 cgaggtatgt aggcggtgct acagagttct tgaagtggtg gcctaactac ggctacacta\n",
       "     4501 gaaggacagt atttggtatc tgcgctctgc tgaagccagt taccttcgga aaaagagttg\n",
       "     4561 gtagctcttg atccggcaaa caaaccaccg ctggtagcgg tggttttttt gtttgcaagc\n",
       "     4621 agcagattac gcgcagaaaa aaaggatctc aagaagatcc tttgatcttt tctacggggt\n",
       "     4681 ctgacgctca gtggaacgaa aactcacgtt aagggatttt ggtcatgagg ggtaataact\n",
       "     4741 gatataatta aattgaagct ctaatttgtg agtttagtat acatgcattt acttataata\n",
       "     4801 cagtttttta gttttgctgg ccgcatcttc tcaaatatgc ttcccagcct gcttttctgt\n",
       "     4861 aacgttcacc ctctacctta gcatcccttc cctttgcaaa tagtcctctt ccaacaataa\n",
       "     4921 taatgtcaga tcctgtagag accacatcat ccacggttct atactgttga cccaatgcgt\n",
       "     4981 ctcccttgtc atctaaaccc acaccgggtg tcataatcaa ccaatcgtaa ccttcatctc\n",
       "     5041 ttccacccat gtctctttga gcaataaagc cgataacaaa atctttgtcg ctcttcgcaa\n",
       "     5101 tgtcaacagt acccttagta tattctccag tagataggga gcccttgcat gacaattctg\n",
       "     5161 ctaacatcaa aaggcctcta ggttcctttg ttacttcttc tgccgcctgc ttcaaaccgc\n",
       "     5221 taacaatacc tgggcccacc acaccgtgtg cattcgtaat gtctgcccat tctgctattc\n",
       "     5281 tgtatacacc cgcagagtac tgcaatttga ctgtattacc aatgtcagca aattttctgt\n",
       "     5341 cttcgaagag taaaaaattg tacttggcgg ataatgcctt tagcggctta actgtgccct\n",
       "     5401 ccatggaaaa atcagtcaaa atatccacat gtgtttttag taaacaaatt ttgggaccta\n",
       "     5461 atgcttcaac taactccagt aattccttgg tggtacgaac atccaatgaa gcacacaagt\n",
       "     5521 ttgtttgctt ttcgtgcatg atattaaata gcttggcagc aacaggacta ggatgagtag\n",
       "     5581 cagcacgttc cttatatgta gctttcgaca tgatttatct tcgtttcctg caggtttttg\n",
       "     5641 ttctgtgcag ttgggttaag aatactgggc aatttcatgt ttcttcaaca ctacatatgc\n",
       "     5701 gtatatatac caatctaagt ctgtgctcct tccttcgttc ttccttctgt tcggagatta\n",
       "     5761 ccgaatcaaa aaaatttcaa agaaaccgaa atcaaaaaaa agaataaaaa aaaaatgatg\n",
       "     5821 aattgaattg aaaagctagc ttatcgatga taagctgtca aagatgagaa ttaattccac\n",
       "     5881 ggactataga ctatactaga tactccgtct actgtacgat acacttccgc tcaggtcctt\n",
       "     5941 gtcctttaac gaggccttac cactcttttg ttactctatt gatccagctc agcaaaggca\n",
       "     6001 gtgtgatcta agattctatc ttcgcgatgt agtaaaacta gctagaccga gaaagagact\n",
       "     6061 agaaatgcaa aaggcacttc tacaatggct gccatcatta ttatccgatg tgacgctgca\n",
       "     6121 gcttctcaat gatattcgaa tacgctttga ggagatacag cctaatatcc gacaaactgt\n",
       "     6181 tttacagatt tacgatcgta cttgttaccc atcattgaat tttgaacatc cgaacctggg\n",
       "     6241 agttttccct gaaacagata gtatatttga acctgtataa taatatatag tctagcgctt\n",
       "     6301 tacggaagac aatgtatgta tttcggttcc tggagaaact attgcatcta ttgcataggt\n",
       "     6361 aatcttgcac gtcgcatccc cggttcattt tctgcgtttc catcttgcac ttcaatagca\n",
       "     6421 tatctttgtt aacgaagcat ctgtgcttca ttttgtagaa caaaaatgca acgcgagagc\n",
       "     6481 gctaattttt caaacaaaga atctgagctg catttttaca gaacagaaat gcaacgcgaa\n",
       "     6541 agcgctattt taccaacgaa gaatctgtgc ttcatttttg taaaacaaaa atgcaacgcg\n",
       "     6601 acgagagcgc taatttttca aacaaagaat ctgagctgca tttttacaga acagaaatgc\n",
       "     6661 aacgcgagag cgctatttta ccaacaaaga atctatactt cttttttgtt ctacaaaaat\n",
       "     6721 gcatcccgag agcgctattt ttctaacaaa gcatcttaga ttactttttt tctcctttgt\n",
       "     6781 gcgctctata atgcagtctc ttgataactt tttgcactgt aggtccgtta aggttagaag\n",
       "     6841 aaggctactt tggtgtctat tttctcttcc ataaaaaaag cctgactcca cttcccgcgt\n",
       "     6901 ttactgatta ctagcgaagc tgcgggtgca ttttttcaag ataaaggcat ccccgattat\n",
       "     6961 attctatacc gatgtggatt gcgcatactt tgtgaacaga aagtgatagc gttgatgatt\n",
       "     7021 cttcattggt cagaaaatta tgaacggttt cttctatttt gtctctatat actacgtata\n",
       "     7081 ggaaatgttt acattttcgt attgttttcg attcactcta tgaatagttc ttactacaat\n",
       "     7141 ttttttgtct aaagagtaat actagagata aacataaaaa atgtagaggt cgagtttaga\n",
       "     7201 tgcaagttca aggagcgaaa ggtggatggg taggttatat agggatatag cacagagata\n",
       "     7261 tatagcaaag agatactttt gagcaatgtt tgtggaagcg gtattcgcaa tgggaagctc\n",
       "     7321 caccccggtt gataatcaga aaagccccaa aaacaggaag attattatca aaaaggatct\n",
       "     7381 tcacctagat ccttttaaat taaaaatgaa gttttaaatc aatctaaagt atatatgagt\n",
       "     7441 aaacttggtc tgacagttac caatgcttaa tcagtgaggc acctatctca gcgatctgtc\n",
       "     7501 tatttcgttc atccatagtt gcctgactcc ccgtcgtgta gataactacg atacgggagc\n",
       "     7561 gcttaccatc tggccccagt gctgcaatga taccgcgaga cccacgctca ccggctccag\n",
       "     7621 atttatcagc aataaaccag ccagccggaa gggccgagcg cagaagtggt cctgcaactt\n",
       "     7681 tatccgcctc catccagtct attaattgtt gccgggaagc tagagtaagt agttcgccag\n",
       "     7741 ttaatagttt gcgcaacgtt gttggcattg ctacaggcat cgtggtgtca ctctcgtcgt\n",
       "     7801 ttggtatggc ttcattcagc tccggttccc aacgatcaag gcgagttaca tgatccccca\n",
       "     7861 tgttgtgcaa aaaagcggtt agctccttcg gtcctccgat cgttgtcaga agtaagttgg\n",
       "     7921 ccgcagtgtt atcactcatg gttatggcag cactgcataa ttctcttact gtcatgccat\n",
       "     7981 ccgtaagatg cttttctgtg actggtgagt actcaaccaa gtcattctga gaatagtgta\n",
       "     8041 tgcggcgacc gagttgctct tgcccggcgt caatacggga taatagtgta tcacatagca\n",
       "     8101 gaactttaaa agtgctcatc attggaaaac gttcttcggg gcgaaaactc tcaaggatct\n",
       "     8161 taccgctgtt gagatccagt tcgatgtaac ccactcgtgc acccaactga tcttcagcat\n",
       "     8221 cttttacttt caccagcgtt tctgggtgag caaaaacagg aaggcaaaat gccgcaaaaa\n",
       "     8281 agggaataag ggcgacacgg aaatgttgaa tactcatact cttccttttt caatattatt\n",
       "     8341 gaagcattta tcagggttat tgtctcatga gcggatacat atttgaatgt atttagaaaa\n",
       "     8401 ataaacaaat aggggttccg cgcacatttc cccgaaaagt gccacctgct aagaaaccat\n",
       "     8461 tattatcatg acattaacct ataaaaatag gcgtatcacg aggccctttc gtc\n",
       "//"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.format(\"gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dseqrecord(o8513)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ape s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bjorn38]",
   "language": "python",
   "name": "conda-env-bjorn38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
